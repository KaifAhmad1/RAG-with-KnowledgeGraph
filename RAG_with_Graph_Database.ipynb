{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "883426dba29646d6b3b02325c851e373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8849ff783707460e880cd7f0acd2041b",
              "IPY_MODEL_5cdd9ea3ff034c5d9f5f78d098bff287",
              "IPY_MODEL_47d917dae9c94dbdbac0103114d26b65"
            ],
            "layout": "IPY_MODEL_dd5f972f71a14a45b61ae9317c2a59cf"
          }
        },
        "8849ff783707460e880cd7f0acd2041b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e8beadada94778b3d83c3a89dde760",
            "placeholder": "​",
            "style": "IPY_MODEL_8b5906c4cc78496ca1573239f9c30d56",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5cdd9ea3ff034c5d9f5f78d098bff287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f55200fc2074164bcd67374686c87f7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc086ad73a684981aab2966a72a55d2f",
            "value": 3
          }
        },
        "47d917dae9c94dbdbac0103114d26b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c4784ae37704af3910561936d437789",
            "placeholder": "​",
            "style": "IPY_MODEL_9ac52541441d4191af7ff79eae4deca5",
            "value": " 3/3 [01:07&lt;00:00, 21.98s/it]"
          }
        },
        "dd5f972f71a14a45b61ae9317c2a59cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e8beadada94778b3d83c3a89dde760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b5906c4cc78496ca1573239f9c30d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f55200fc2074164bcd67374686c87f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc086ad73a684981aab2966a72a55d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c4784ae37704af3910561936d437789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac52541441d4191af7ff79eae4deca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/RAG-with-KnowledgeGraph/blob/main/RAG_with_Graph_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Dependencies:**"
      ],
      "metadata": {
        "id": "98YkxTMk8MkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pw24LkQ27bML"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    transformers \\\n",
        "    langchain \\\n",
        "    huggingface_hub \\\n",
        "    tiktoken \\\n",
        "    neo4j \\\n",
        "    python-dotenv \\\n",
        "    accelerate \\\n",
        "    sentence_transformers \\\n",
        "    bitsandbytes \\\n",
        "    unstructured \\\n",
        "    unstructured[pdf]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "wu99tQMp8cYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting API in Environment Variable:**"
      ],
      "metadata": {
        "id": "ec1_j3db9-On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "load_dotenv()\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://8379a056.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = 'DVimhqZNEXnU2RYsU7bgwc5tAa6zrahYPtdaeCZ3jyc'\n",
        "hf_auth = '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'\n",
        "os.environ['NEO4J_URL'] = \"bolt://server_ip:7687\"\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TV8qynpS8ndS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Model in Notebook:**"
      ],
      "metadata": {
        "id": "k1HiUGtY-hVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "model_id = 'Deci/DeciLM-7B'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Wb_P1tFB86E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# begin initializing HF items, you need an access token\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "8-5HISb4-qgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BnB Configuration\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "C45XOIQEAVDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "883426dba29646d6b3b02325c851e373",
            "8849ff783707460e880cd7f0acd2041b",
            "5cdd9ea3ff034c5d9f5f78d098bff287",
            "47d917dae9c94dbdbac0103114d26b65",
            "dd5f972f71a14a45b61ae9317c2a59cf",
            "d6e8beadada94778b3d83c3a89dde760",
            "8b5906c4cc78496ca1573239f9c30d56",
            "2f55200fc2074164bcd67374686c87f7",
            "dc086ad73a684981aab2966a72a55d2f",
            "8c4784ae37704af3910561936d437789",
            "9ac52541441d4191af7ff79eae4deca5"
          ]
        },
        "id": "JSb__D2C-uMx",
        "outputId": "c62a4120-119c-410c-c456-b8f3565d4e93"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "883426dba29646d6b3b02325c851e373"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:neo4j.io:Failed to write data to connection ResolvedIPv4Address(('34.126.114.186', 7687)) (ResolvedIPv4Address(('34.126.114.186', 7687)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How model looks like:\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "nILHQLiu-zIs",
        "outputId": "98b20f3b-1f31-46fe-b294-56646aa01b98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeciLMForCausalLM(\n",
              "  (model): DeciLMModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-4): 5 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (5-9): 5 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (10): DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (11-19): 9 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (20-30): 11 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=128, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=128, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (31): DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "HoHZiP3SFJHE",
        "outputId": "91009079-67e3-4a4e-8597-82028d6c140a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of strings representing stop signals or markers\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "YTICB6xsGxw-",
        "outputId": "63edbcca-4f63-48ed-ad61-0567c2c051e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 28705, 13, 28769, 6366, 28747], [1, 28705, 13, 13940, 28832, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert token IDs to LongTensor objects\n",
        "import torch\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "7p6mgjI9HOcD",
        "outputId": "f4133134-f9f1-4011-85c6-83923980351c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 28705,    13, 28769,  6366, 28747], device='cuda:0'),\n",
              " tensor([    1, 28705,    13, 13940, 28832,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopping Criteria for Transformer Training:**"
      ],
      "metadata": {
        "id": "XNiM5KxWH9qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# Define a custom stopping criteria class\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.equal(input_ids[0][-len(stop_ids):], stop_ids):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "UeHmT-GzHerh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Huggingface Pipeline:**"
      ],
      "metadata": {
        "id": "4DRI1tSuOPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up text generation pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.3,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "g3A-YXDjHrHu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_text(\"What are the primary mechanisms underlying antibiotic resistance, and how can we develop strategies to combat it?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uTRpa0_mInOA",
        "outputId": "6d5c7f6f-81b4-41de-990b-cdc7608b63ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'What are the primary mechanisms underlying antibiotic resistance, and how can we develop strategies to combat it?\\n2. How do antibiotics work, and what are their side effects?\\n3. What is the difference between bacterial and viral infections, and how do they affect our bodies differently?\\n4. What are some common misconceptions about antibiotics, and how can we correct them?\\n5. What are some natural alternatives to antibiotics that can be used to treat infections? '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm(prompt=\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "Uh3unyirTtTa",
        "outputId": "04d54a35-7ee4-48f7-a1db-a6544140cee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe CRISPR/Cas9 system is a powerful tool for genome editing, but it has limitations. One major limitation is that the Cas9 enzyme can cut DNA at unintended sites in the genome, which can lead to unintended mutations. This phenomenon is known as \"off-target\" activity. Off-target activity can be minimized by using a guide RNA (gRNA) with high specificity for the target site, but this approach is not always possible. In addition, the Cas9 enzyme is relatively large and difficult to deliver into cells. These limitations have limited the use of CRISPR/Cas9 for therapeutic applications.\\nIn this project, we will develop new methods to improve the specificity and efficiency of CRISPR/Cas9 gene-editing technology. We will test these methods in human cells and animal models. Our goal is to develop a safe and effective method for treating genetic diseases caused by single-nucleotide mutations.\\nShowing the most recent 10 out of 23 publications '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Document Data:**"
      ],
      "metadata": {
        "id": "kBNYzCfyOJRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('/content/drive/MyDrive/BioMedical-Dataset', glob=\"**/*.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "knhECj2bOUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))"
      ],
      "metadata": {
        "id": "T0Yp2a8Goeqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                               chunk_overlap=30,\n",
        "                                               add_start_index=True,\n",
        "                                               separators=MARKDOWN_SEPARATORS)\n",
        "\n",
        "processed_text_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "6Tznkr3PIawL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text_splits[120].page_content"
      ],
      "metadata": {
        "id": "91wesl_N4yYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(processed_text_splits))"
      ],
      "metadata": {
        "id": "El9WlHtHCiTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Embdeddings of the sentences and storing it into Graph DB\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "KqOudIQtIayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Neo4j Graph:**"
      ],
      "metadata": {
        "id": "WaGxQILaCwQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ],
      "metadata": {
        "id": "v__99eeVDVdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a new custom Index using Cypher:**"
      ],
      "metadata": {
        "id": "KasRmCh0bWJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create New index with custom embedding model and dimensions\n",
        "# I have already created\n",
        "'''\n",
        "graph.query(\"\"\"\n",
        "CALL db.index.vector.createNodeIndex(\n",
        "  'KG-Enhanced-QnA-Biomedical',\n",
        "  'text_splits',\n",
        "  'embeddings',\n",
        "   768,\n",
        "   'cosine'\n",
        ")\n",
        "\"\"\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Lhzqh0uKbVJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show Created Vector Index:**"
      ],
      "metadata": {
        "id": "z3ayEjLyHTAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "uri = os.environ[\"NEO4J_URI\"]\n",
        "username = os.environ[\"NEO4J_USERNAME\"]\n",
        "password = os.environ[\"NEO4J_PASSWORD\"]\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "session = driver.session()\n",
        "\n",
        "result = session.run(\"SHOW VECTOR INDEXES\")\n",
        "\n",
        "for record in result:\n",
        "   print(record)"
      ],
      "metadata": {
        "id": "SFLExy9OxptZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' chunks = [{'text': document.page_content, 'embedding': embeddings.embed_query(document.page_content)}\n",
        "          for document in documents if len(document.page_content) >  50]  '''"
      ],
      "metadata": {
        "id": "ii7wVWOmJYBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "graph.query(\"\"\"\n",
        "UNWIND $data AS row\n",
        "CREATE (c:Chunk {text: row.text})\n",
        "WITH c, row\n",
        "CALL db.create.setVectorProperty(c, 'embedding', row.embedding)\n",
        "YIELD node\n",
        "RETURN distinct 'done'\n",
        "\"\"\", {'data': chunks})\n",
        "'''"
      ],
      "metadata": {
        "id": "B0KQnPT8Lxqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "vector_search = \"\"\"\n",
        "WITH $embedding AS e\n",
        "CALL db.index.vector.queryNodes('KG-Enhanced-QnA-Biomedical',$k, e) yield node, score\n",
        "RETURN node.text AS result\n",
        "ORDER BY score DESC\n",
        "LIMIT 3\n",
        "\"\"\"\n",
        "'''"
      ],
      "metadata": {
        "id": "0Tbwl91LjKd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Neo4j vector from documents\n",
        "neo4j_vector = Neo4jVector.from_documents(\n",
        "    processed_text_splits,\n",
        "    embeddings,\n",
        "    index_name='KG-Enhanced-QnA-Biomedical',\n",
        "     url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ],
      "metadata": {
        "id": "HHjtYv6JDVfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Similarity Search\n",
        "query = \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
        "vector_results = neo4j_vector.similarity_search(query, k=2)\n",
        "\n",
        "for i, res in enumerate(vector_results):\n",
        "    print(res.page_content)\n",
        "    if i != len(vector_results) - 1:\n",
        "        print()\n",
        "vector_result = vector_results[0].page_content"
      ],
      "metadata": {
        "id": "UAV-XTaHJIjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.graphs import Neo4jGraph"
      ],
      "metadata": {
        "id": "Az7n09kuW4k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.base import Chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
        "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
        "from typing import Any, Dict, List\n",
        "from pydantic import Field"
      ],
      "metadata": {
        "id": "uE_4FkZtMyaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_search = \"\"\"\n",
        "WITH $embedding AS e\n",
        "CALL db.index.vector.queryNodes('KG-Enhanced-QnA-Biomedical',$k, e) yield node, score\n",
        "RETURN node.text AS result\n",
        "ORDER BY score DESC\n",
        "LIMIT 3\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jqnt60GeNB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph.schema)"
      ],
      "metadata": {
        "id": "yf3jH9ZGXPUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neo4jVectorChain(Chain):\n",
        "    graph: Neo4jGraph = Field(exclude=True)\n",
        "    input_key: str = \"query\"\n",
        "    output_key: str = \"result\"\n",
        "    embeddings: HuggingFaceBgeEmbeddings = HuggingFaceBgeEmbeddings()\n",
        "    qa_chain: LLMChain = LLMChain(llm=llm, prompt=CHAT_PROMPT)\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        return [self.input_key]\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        _output_keys = [self.output_key]\n",
        "        return _output_keys\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str], run_manager, k=3) -> Dict[str, Any]:\n",
        "        question = inputs[self.input_key]\n",
        "        embedding = self.embeddings.embed_query(question)\n",
        "\n",
        "        context = self.graph.query(vector_search, {'embedding': embedding, 'k': 3})\n",
        "        context = [el['result'] for el in context]\n",
        "\n",
        "        result = self.qa_chain({\"question\": question, \"context\": context})\n",
        "        final_result = result[self.qa_chain.output_key]\n",
        "        return {self.output_key: final_result}"
      ],
      "metadata": {
        "id": "wYZQ44hsNT4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = Neo4jVectorChain(graph=graph, embeddings=embeddings, verbose=True)"
      ],
      "metadata": {
        "id": "tksLTpsqOGGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_result = chain.invoke(\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "TS4Nwf6pONk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = GraphCypherQAChain.from_llm(\n",
        "    cypher_llm=llm,\n",
        "    qa_llm=llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    return_intermediate_steps=True,\n",
        "    validate_cypher=True\n",
        ")"
      ],
      "metadata": {
        "id": "2Q-g7wGhXR0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_result = chain.invoke(\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "JUxlLbCEXczj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0W8nO07iOmq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}