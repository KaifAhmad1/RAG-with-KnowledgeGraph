{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b7974ef1450427ab82684852de0cded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8052b400a9d64f54a74ed3e3a9009430",
              "IPY_MODEL_cb43ad16e19f4d9495a9d0e9294247fb",
              "IPY_MODEL_0f1b0a23169545adb257bacb2f179dd0"
            ],
            "layout": "IPY_MODEL_2492d7046410426c9bb99d6acef451dd"
          }
        },
        "8052b400a9d64f54a74ed3e3a9009430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69d2f09971b84a9f813b3e82fcb31a37",
            "placeholder": "​",
            "style": "IPY_MODEL_9b138f51582c47feb5f94af48a9cd217",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cb43ad16e19f4d9495a9d0e9294247fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_126c351dbbb3487986f7a0af8cc53bfd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c34731679f02419daa6fecf2da24b372",
            "value": 3
          }
        },
        "0f1b0a23169545adb257bacb2f179dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e17e69d827cd470887f5e4515e518d54",
            "placeholder": "​",
            "style": "IPY_MODEL_a7d3e12413fd4fa9823d6b7f1e10790f",
            "value": " 3/3 [01:24&lt;00:00, 27.64s/it]"
          }
        },
        "2492d7046410426c9bb99d6acef451dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d2f09971b84a9f813b3e82fcb31a37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b138f51582c47feb5f94af48a9cd217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "126c351dbbb3487986f7a0af8cc53bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c34731679f02419daa6fecf2da24b372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e17e69d827cd470887f5e4515e518d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7d3e12413fd4fa9823d6b7f1e10790f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/RAG-with-KnowledgeGraph/blob/main/RAG_with_Graph_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Dependencies:**"
      ],
      "metadata": {
        "id": "98YkxTMk8MkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pw24LkQ27bML"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    transformers \\\n",
        "    langchain \\\n",
        "    huggingface_hub \\\n",
        "    tiktoken \\\n",
        "    neo4j \\\n",
        "    python-dotenv \\\n",
        "    accelerate \\\n",
        "    sentence_transformers \\\n",
        "    bitsandbytes \\\n",
        "    unstructured \\\n",
        "    unstructured[pdf]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "wu99tQMp8cYs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting API in Environment Variable:**"
      ],
      "metadata": {
        "id": "ec1_j3db9-On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "load_dotenv()\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://8379a056.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = 'DVimhqZNEXnU2RYsU7bgwc5tAa6zrahYPtdaeCZ3jyc'\n",
        "hf_auth = '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'\n",
        "os.environ['NEO4J_URL'] = \"bolt://server_ip:7687\"\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TV8qynpS8ndS",
        "outputId": "0d8b8d83-11da-4798-ead4-cb753f1dd73e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Model in Notebook:**"
      ],
      "metadata": {
        "id": "k1HiUGtY-hVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "model_id = 'Deci/DeciLM-7B'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Wb_P1tFB86E4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# begin initializing HF items, you need an access token\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "8-5HISb4-qgq",
        "outputId": "2aceb033-4198-4f77-9614-15b3ce6e6bf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:1085: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BnB Configuration\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "C45XOIQEAVDz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "2b7974ef1450427ab82684852de0cded",
            "8052b400a9d64f54a74ed3e3a9009430",
            "cb43ad16e19f4d9495a9d0e9294247fb",
            "0f1b0a23169545adb257bacb2f179dd0",
            "2492d7046410426c9bb99d6acef451dd",
            "69d2f09971b84a9f813b3e82fcb31a37",
            "9b138f51582c47feb5f94af48a9cd217",
            "126c351dbbb3487986f7a0af8cc53bfd",
            "c34731679f02419daa6fecf2da24b372",
            "e17e69d827cd470887f5e4515e518d54",
            "a7d3e12413fd4fa9823d6b7f1e10790f"
          ]
        },
        "id": "JSb__D2C-uMx",
        "outputId": "c456359e-4e30-4985-8722-ed7325b7ec6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b7974ef1450427ab82684852de0cded"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How model looks like:\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "nILHQLiu-zIs",
        "outputId": "a4ea3f5b-e17a-4ee3-c3de-b5ba1ff9a2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeciLMForCausalLM(\n",
              "  (model): DeciLMModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-4): 5 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (5-9): 5 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (10): DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (11-19): 9 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=256, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (20-30): 11 x DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=128, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=128, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "      (31): DeciLMDecoderLayer(\n",
              "        (self_attn): DeciLMAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "HoHZiP3SFJHE",
        "outputId": "9ca6958f-2b5a-4bec-c84c-1d91e8e091a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:711: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of strings representing stop signals or markers\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "YTICB6xsGxw-",
        "outputId": "0c36eddd-f0d1-4d3e-fcae-3f992017b38c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 28705, 13, 28769, 6366, 28747], [1, 28705, 13, 13940, 28832, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert token IDs to LongTensor objects\n",
        "import torch\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "7p6mgjI9HOcD",
        "outputId": "b40a8480-d1b2-46c5-e0ea-5d199d3f25dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 28705,    13, 28769,  6366, 28747], device='cuda:0'),\n",
              " tensor([    1, 28705,    13, 13940, 28832,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopping Criteria for Transformer Training:**"
      ],
      "metadata": {
        "id": "XNiM5KxWH9qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# Define a custom stopping criteria class\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.equal(input_ids[0][-len(stop_ids):], stop_ids):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "UeHmT-GzHerh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Huggingface Pipeline:**"
      ],
      "metadata": {
        "id": "4DRI1tSuOPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up text generation pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.3,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "g3A-YXDjHrHu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_text(\"What are the primary mechanisms underlying antibiotic resistance, and how can we develop strategies to combat it?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uTRpa0_mInOA",
        "outputId": "7326adee-9e7b-473b-d682-f16c72f0aea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'What are the primary mechanisms underlying antibiotic resistance, and how can we develop strategies to combat it?\\n2. How do antibiotics work, and what are their side effects?\\n3. What is the difference between bacterial and viral infections, and how do they affect our bodies differently?\\n4. What are some common misconceptions about antibiotics, and how can we correct them?\\n5. What are some natural alternatives to antibiotics that can be used to treat infections? '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm(prompt=\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "Uh3unyirTtTa",
        "outputId": "f90f9aa5-3317-42eb-aebe-8bd2525d611b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe CRISPR/Cas9 system is a powerful tool for genome editing, but it has limitations. One major limitation is that the Cas9 enzyme can cut DNA at unintended sites in the genome, which can lead to unintended mutations. This phenomenon is known as \"off-target\" activity. Off-target activity can be minimized by using a guide RNA (gRNA) with high specificity for the target site, but this approach is not always possible. In addition, the Cas9 enzyme is relatively large and difficult to deliver into cells. These limitations have limited the use of CRISPR/Cas9 for therapeutic applications.\\nIn this project, we will develop new methods to improve the specificity and efficiency of CRISPR/Cas9 gene-editing technology. We will test these methods in human cells and animal models. Our goal is to develop a safe and effective method for treating genetic diseases caused by single-nucleotide mutations.\\nShowing the most recent 10 out of 23 publications '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Document Data:**"
      ],
      "metadata": {
        "id": "kBNYzCfyOJRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('/content/drive/MyDrive/BioMedical-Dataset', glob=\"**/*.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "knhECj2bOUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))"
      ],
      "metadata": {
        "id": "T0Yp2a8Goeqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=30)\n",
        "text_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "6Tznkr3PIawL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splits[120].page_content"
      ],
      "metadata": {
        "id": "91wesl_N4yYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_splits))"
      ],
      "metadata": {
        "id": "El9WlHtHCiTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Embdeddings of the sentences and storing it into Graph DB\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "KqOudIQtIayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Neo4j Graph:**"
      ],
      "metadata": {
        "id": "WaGxQILaCwQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ],
      "metadata": {
        "id": "v__99eeVDVdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a new custom Index using Cypher:**"
      ],
      "metadata": {
        "id": "KasRmCh0bWJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create New index with custom embedding model and dimensions\n",
        "graph.query(\"\"\"\n",
        "CALL db.index.vector.createNodeIndex(\n",
        "  'KG-Enhanced-QnA-Biomedical',\n",
        "  'text_splits',\n",
        "  'embeddings',\n",
        "   768,\n",
        "   'cosine'\n",
        ")\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "Lhzqh0uKbVJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show Created Vector Index:**"
      ],
      "metadata": {
        "id": "z3ayEjLyHTAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "uri = os.environ[\"NEO4J_URI\"]\n",
        "username = os.environ[\"NEO4J_USERNAME\"]\n",
        "password = os.environ[\"NEO4J_PASSWORD\"]\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "session = driver.session()\n",
        "\n",
        "result = session.run(\"SHOW VECTOR INDEXES\")\n",
        "\n",
        "for record in result:\n",
        "   print(record)"
      ],
      "metadata": {
        "id": "SFLExy9OxptZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = [{'text': document.page_content, 'embedding': embeddings.embed_query(document.page_content)}\n",
        "          for document in documents if len(document.page_content) >  50]"
      ],
      "metadata": {
        "id": "ii7wVWOmJYBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(\"\"\"\n",
        "UNWIND $data AS row\n",
        "CREATE (c:Chunk {text: row.text})\n",
        "WITH c, row\n",
        "CALL db.create.setVectorProperty(c, 'embedding', row.embedding)\n",
        "YIELD node\n",
        "RETURN distinct 'done'\n",
        "\"\"\", {'data': chunks})"
      ],
      "metadata": {
        "id": "B0KQnPT8Lxqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.base import Chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
        "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
        "\n",
        "from typing import Any, Dict, List\n",
        "from pydantic import Field"
      ],
      "metadata": {
        "id": "QUb_l8QdioKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_search = \"\"\"\n",
        "WITH $embedding AS e\n",
        "CALL db.index.vector.queryNodes('KG-Enhanced-QnA-Biomedical',$k, e) yield node, score\n",
        "RETURN node.text AS result\n",
        "ORDER BY score DESC\n",
        "LIMIT 3\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0Tbwl91LjKd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Neo4j vector from documents\n",
        "neo4j_vector = Neo4jVector.from_documents(\n",
        "    text_splits,\n",
        "    embeddings,\n",
        "    index_name='KG-Enhanced-QnA-Biomedical',\n",
        "     url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ],
      "metadata": {
        "id": "HHjtYv6JDVfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Similarity Search\n",
        "query = \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
        "vector_results = neo4j_vector.similarity_search(query, k=2)\n",
        "\n",
        "for i, res in enumerate(vector_results):\n",
        "    print(res.page_content)\n",
        "    if i != len(vector_results) - 1:\n",
        "        print()\n",
        "vector_result = vector_results[0].page_content"
      ],
      "metadata": {
        "id": "UAV-XTaHJIjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}