{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68d50dc40e4f41fd8ec38e651cf0a7f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdf411cc13334c018c00c6882b10fa46",
              "IPY_MODEL_73f9d7649c184eb1ac411709b73b6f00",
              "IPY_MODEL_5c253edf283b422ba88c1aca01382729"
            ],
            "layout": "IPY_MODEL_ef69b51ee03d4febb313e83f7759095b"
          }
        },
        "fdf411cc13334c018c00c6882b10fa46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c61fcda399364c13adda3d9303593c4d",
            "placeholder": "​",
            "style": "IPY_MODEL_6bc9f3ce38aa45d8870348fcd4812d84",
            "value": "config.json: 100%"
          }
        },
        "73f9d7649c184eb1ac411709b73b6f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2973f0531a85467fab087f2bc0b332dd",
            "max": 863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_510992c1c0fc4cbcbe1905d3b63f6e14",
            "value": 863
          }
        },
        "5c253edf283b422ba88c1aca01382729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f7b4c5ac924a1a9c28e9a243bf0759",
            "placeholder": "​",
            "style": "IPY_MODEL_a9cbfbfc91b649f3b73d06032eea13dc",
            "value": " 863/863 [00:00&lt;00:00, 17.7kB/s]"
          }
        },
        "ef69b51ee03d4febb313e83f7759095b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61fcda399364c13adda3d9303593c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc9f3ce38aa45d8870348fcd4812d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2973f0531a85467fab087f2bc0b332dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "510992c1c0fc4cbcbe1905d3b63f6e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24f7b4c5ac924a1a9c28e9a243bf0759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9cbfbfc91b649f3b73d06032eea13dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/RAG-with-KnowledgeGraph/blob/main/RAG_with_Graph_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Dependencies:**"
      ],
      "metadata": {
        "id": "98YkxTMk8MkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pw24LkQ27bML",
        "outputId": "7fbd24ab-7b6b-42f3-eeac-36cafc808398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.3/241.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m133.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting qU\n",
            "  Downloading qu-1.2.5.tar.gz (3.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensor_parallel\n",
            "  Downloading tensor_parallel-2.0.0-py3-none-any.whl (37 kB)\n",
            "Collecting qiniu>=7.2.0 (from qU)\n",
            "  Downloading qiniu-7.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=6.7 in /usr/local/lib/python3.10/dist-packages (from qU) (8.1.7)\n",
            "Requirement already satisfied: pytest>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from qU) (7.4.4)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from tensor_parallel) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.20.1 in /usr/local/lib/python3.10/dist-packages (from tensor_parallel) (4.37.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3.3.1->qU) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest>=3.3.1->qU) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.3.1->qU) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.3.1->qU) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=3.3.1->qU) (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qiniu>=7.2.0->qU) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->tensor_parallel) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.20.1->tensor_parallel) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->tensor_parallel) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qiniu>=7.2.0->qU) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->qiniu>=7.2.0->qU) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qiniu>=7.2.0->qU) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->qiniu>=7.2.0->qU) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->tensor_parallel) (1.3.0)\n",
            "Building wheels for collected packages: qU\n",
            "  Building wheel for qU (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qU: filename=qu-1.2.5-py3-none-any.whl size=3821 sha256=aaa669efc24e145c8f38981f0744a883eefd6f40ab164774ded72e1a9877fa15\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/67/6f/f135053c2417e6f02eba0d670d9468c23a0c38c8ad404854cf\n",
            "Successfully built qU\n",
            "Installing collected packages: qiniu, qU, tensor_parallel\n",
            "Successfully installed qU-1.2.5 qiniu-7.13.0 tensor_parallel-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU transformers\n",
        "!pip install -qU langchain\n",
        "!pip install -qU huggingface_hub\n",
        "!pip install -qU tiktoken\n",
        "!pip install -qU neo4j\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU accelerate\n",
        "!pip install -qU sentence_transformers\n",
        "!pip install -qU  bitsandbytes\n",
        "!pip install -qU  optimum\n",
        "!pip install -qU unstructured unstructured[pdf]\n",
        "!pip install qU tensor_parallel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "wu99tQMp8cYs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting API in Environment Variable:**"
      ],
      "metadata": {
        "id": "ec1_j3db9-On"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "load_dotenv()\n",
        "os.environ[\"NEO4J_URI\"] = 'neo4j+s://d5dffe81.databases.neo4j.io'\n",
        "os.environ[\"NEO4J_USERNAME\"] = 'neo4j'\n",
        "os.environ[\"NEO4J_PASSWORD\"] = 'C8A_mt9s8yar3i44Xi1bVbkrFVK3aCXE1w5cQvHv6LM'\n",
        "hf_auth = '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'\n",
        "os.environ['NEO4J_URL'] = \"bolt://server_ip:7687\"\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TV8qynpS8ndS",
        "outputId": "1f505ef7-c19e-41a9-afa9-af18eba2dd77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Model in Notebook:**"
      ],
      "metadata": {
        "id": "k1HiUGtY-hVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "import tensor_parallel as tp\n",
        "model_id = 'microsoft/phi-2'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Wb_P1tFB86E4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# begin initializing HF items, you need an access token\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "8-5HISb4-qgq",
        "outputId": "376cc98f-47ff-40a9-de26-627ec86f56a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "68d50dc40e4f41fd8ec38e651cf0a7f4",
            "fdf411cc13334c018c00c6882b10fa46",
            "73f9d7649c184eb1ac411709b73b6f00",
            "5c253edf283b422ba88c1aca01382729",
            "ef69b51ee03d4febb313e83f7759095b",
            "c61fcda399364c13adda3d9303593c4d",
            "6bc9f3ce38aa45d8870348fcd4812d84",
            "2973f0531a85467fab087f2bc0b332dd",
            "510992c1c0fc4cbcbe1905d3b63f6e14",
            "24f7b4c5ac924a1a9c28e9a243bf0759",
            "a9cbfbfc91b649f3b73d06032eea13dc"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:1085: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68d50dc40e4f41fd8ec38e651cf0a7f4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BnB Configuration\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "C45XOIQEAVDz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=model_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "JSb__D2C-uMx",
        "outputId": "6ea95c32-7c19-4b1c-a929-4a395ece1028"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No GPU found. A GPU is needed for quantization.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3c62d46097f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = transformers.AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_auth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m                 raise ImportError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How model looks like:\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "nILHQLiu-zIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "HoHZiP3SFJHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bytes_to_giga_bytes(bytes):\n",
        "  return bytes / 1024 / 1024 / 1024"
      ],
      "metadata": {
        "id": "_725TDgc6oit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
      ],
      "metadata": {
        "id": "-6YYkQq_6sgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "def flush():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "id": "G6J7mgxF7Jit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flush()"
      ],
      "metadata": {
        "id": "nnbBYqzc7ZBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of strings representing stop signals or markers\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "YTICB6xsGxw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert token IDs to LongTensor objects\n",
        "import torch\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "7p6mgjI9HOcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopping Criteria for Transformer Training:**"
      ],
      "metadata": {
        "id": "XNiM5KxWH9qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# Define a custom stopping criteria class\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.equal(input_ids[0][-len(stop_ids):], stop_ids):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "UeHmT-GzHerh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Huggingface Pipeline:**"
      ],
      "metadata": {
        "id": "4DRI1tSuOPpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up text generation pipeline\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.3,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "g3A-YXDjHrHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_text(\"What are the primary mechanisms underlying antibiotic resistance, and how can we develop strategies to combat it?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uTRpa0_mInOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm(prompt=\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "Uh3unyirTtTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Document Data:**"
      ],
      "metadata": {
        "id": "kBNYzCfyOJRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('/content/drive/MyDrive/BioMedical-Dataset', glob=\"**/*.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "knhECj2bOUkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))"
      ],
      "metadata": {
        "id": "T0Yp2a8Goeqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                               chunk_overlap=30,\n",
        "                                               add_start_index=True,\n",
        "                                               separators=MARKDOWN_SEPARATORS)\n",
        "\n",
        "processed_text_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "6Tznkr3PIawL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text_splits[120].page_content"
      ],
      "metadata": {
        "id": "91wesl_N4yYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(processed_text_splits))"
      ],
      "metadata": {
        "id": "El9WlHtHCiTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Embdeddings of the sentences and storing it into Graph DB\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "KqOudIQtIayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Neo4j Graph:**"
      ],
      "metadata": {
        "id": "WaGxQILaCwQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ],
      "metadata": {
        "id": "v__99eeVDVdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a new custom Index using Cypher:**"
      ],
      "metadata": {
        "id": "KasRmCh0bWJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create New index with custom embedding model and dimensions\n",
        "# I have already created\n",
        "'''\n",
        "graph.query(\"\"\"\n",
        "CALL db.index.vector.createNodeIndex(\n",
        "  'KG-Enhanced-QnA-Biomedical',\n",
        "  'text_splits',\n",
        "  'embeddings',\n",
        "   768,\n",
        "   'cosine'\n",
        ")\n",
        "\"\"\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Lhzqh0uKbVJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show Created Vector Index:**"
      ],
      "metadata": {
        "id": "z3ayEjLyHTAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "uri = os.environ[\"NEO4J_URI\"]\n",
        "username = os.environ[\"NEO4J_USERNAME\"]\n",
        "password = os.environ[\"NEO4J_PASSWORD\"]\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "session = driver.session()\n",
        "\n",
        "result = session.run(\"SHOW VECTOR INDEXES\")\n",
        "\n",
        "for record in result:\n",
        "   print(record)"
      ],
      "metadata": {
        "id": "SFLExy9OxptZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' chunks = [{'text': document.page_content, 'embedding': embeddings.embed_query(document.page_content)}\n",
        "          for document in documents if len(document.page_content) >  50]  '''"
      ],
      "metadata": {
        "id": "ii7wVWOmJYBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "graph.query(\"\"\"\n",
        "UNWIND $data AS row\n",
        "CREATE (c:Chunk {text: row.text})\n",
        "WITH c, row\n",
        "CALL db.create.setVectorProperty(c, 'embedding', row.embedding)\n",
        "YIELD node\n",
        "RETURN distinct 'done'\n",
        "\"\"\", {'data': chunks})\n",
        "'''"
      ],
      "metadata": {
        "id": "B0KQnPT8Lxqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "vector_search = \"\"\"\n",
        "WITH $embedding AS e\n",
        "CALL db.index.vector.queryNodes('KG-Enhanced-QnA-Biomedical',$k, e) yield node, score\n",
        "RETURN node.text AS result\n",
        "ORDER BY score DESC\n",
        "LIMIT 3\n",
        "\"\"\"\n",
        "'''"
      ],
      "metadata": {
        "id": "0Tbwl91LjKd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Neo4j vector from documents\n",
        "neo4j_vector = Neo4jVector.from_documents(\n",
        "    processed_text_splits,\n",
        "    embeddings,\n",
        "    index_name='KG-Enhanced-QnA-Biomedical',\n",
        "     url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ],
      "metadata": {
        "id": "HHjtYv6JDVfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Similarity Search\n",
        "query = \"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\"\n",
        "vector_results = neo4j_vector.similarity_search(query, k=2)\n",
        "\n",
        "for i, res in enumerate(vector_results):\n",
        "    print(res.page_content)\n",
        "    if i != len(vector_results) - 1:\n",
        "        print()\n",
        "vector_result = vector_results[0].page_content"
      ],
      "metadata": {
        "id": "UAV-XTaHJIjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.graphs import Neo4jGraph"
      ],
      "metadata": {
        "id": "Az7n09kuW4k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.base import Chain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT\n",
        "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
        "from typing import Any, Dict, List\n",
        "from pydantic import Field"
      ],
      "metadata": {
        "id": "uE_4FkZtMyaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_search = \"\"\"\n",
        "WITH $embedding AS e\n",
        "CALL db.index.vector.queryNodes('KG-Enhanced-QnA-Biomedical',$k, e) yield node, score\n",
        "RETURN node.text AS result\n",
        "ORDER BY score DESC\n",
        "LIMIT 3\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jqnt60GeNB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph.schema)"
      ],
      "metadata": {
        "id": "yf3jH9ZGXPUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neo4jVectorChain(Chain):\n",
        "    graph: Neo4jGraph = Field(exclude=True)\n",
        "    input_key: str = \"query\"\n",
        "    output_key: str = \"result\"\n",
        "    embeddings: HuggingFaceBgeEmbeddings = HuggingFaceBgeEmbeddings()\n",
        "    qa_chain: LLMChain = LLMChain(llm=llm, prompt=CHAT_PROMPT)\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        return [self.input_key]\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        _output_keys = [self.output_key]\n",
        "        return _output_keys\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str], run_manager, k=3) -> Dict[str, Any]:\n",
        "        question = inputs[self.input_key]\n",
        "        embedding = self.embeddings.embed_query(question)\n",
        "\n",
        "        context = self.graph.query(vector_search, {'embedding': embedding, 'k': 3})\n",
        "        context = [el['result'] for el in context]\n",
        "\n",
        "        result = self.qa_chain({\"question\": question, \"context\": context})\n",
        "        final_result = result[self.qa_chain.output_key]\n",
        "        return {self.output_key: final_result}"
      ],
      "metadata": {
        "id": "wYZQ44hsNT4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = Neo4jVectorChain(graph=graph, embeddings=embeddings, verbose=True)"
      ],
      "metadata": {
        "id": "tksLTpsqOGGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_result = chain.run(\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "TS4Nwf6pONk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = GraphCypherQAChain.from_llm(\n",
        "    cypher_llm=llm,\n",
        "    qa_llm=llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    return_intermediate_steps=True,\n",
        "    validate_cypher=True\n",
        ")"
      ],
      "metadata": {
        "id": "2Q-g7wGhXR0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_result = chain.run(\"How can we enhance the specificity and efficiency of CRISPR/Cas9 gene-editing technology to minimize off-target effects and increase its potential for therapeutic applications?\")"
      ],
      "metadata": {
        "id": "JUxlLbCEXczj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0zY2J3pvwNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}